{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c7e6cd-6087-42eb-add7-0143210de63e",
   "metadata": {},
   "source": [
    "# Deconstructing QWen2 from the Ground Up\n",
    "In this project, I will demonstrate how to deconstruct QWen2 from scratch. Specifically, I will explore how to complete a Chinese proverb which is generating a \"退\" from the input input_text=\"学习如逆水行舟，不进则\". I hope this project will help everyone gain a better understanding of the structure of QWen2, and also want to take this opportunity to promote China's LLM.\n",
    "\n",
    "Here is the offical link to download the weights: **https://www.modelscope.cn/models/qwen/Qwen2-7B/files**\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/all_steps.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d46041-7dbd-4d7c-bb14-2d7bbd7d9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3846ceb-a2d3-4104-9b8e-c1c56f6fd115",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "Here, I'm not going to show the principle and implementation of LLM's tokenizer. Andrej Karpathy has provided a one-to-one implementation of GPT4Tokenizer. His code is really easy to understand!!!\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/tokenizer.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3116bf7-4777-4a76-be81-7ff8c52b1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520929d795234c9581ad98d167eaeaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To ensure every layer's output is same with model.generate(). The model should be load in precision of torch.float32!\n",
    "model_path=\"Qwen/Qwen2-7B\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331a2b5-a2ba-4fbf-9f05-036a538313a6",
   "metadata": {},
   "source": [
    "# Reading the model file\n",
    "Normally, we can use LLM to inference text by run the entire model.\n",
    "However, this project is going to show the structure of QWen2, so I will run the martrix in model layer by layer.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/layers_dict.png\" width=500/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fda03f-4ff6-4729-8f14-cf4ea4af5bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.bias\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.bias\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.bias\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.bias\",\n",
      "    \"model.layers.1.self_attn.k_proj.weight\",\n",
      "    \"model.layers.1.self_attn.k_proj.bias\",\n",
      "    \"model.layers.1.self_attn.v_proj.weight\",\n",
      "    \"model.layers.1.self_attn.v_proj.bias\",\n",
      "    \"model.layers.1.self_attn.o_proj.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = model.state_dict()\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22de6dab-0872-4adc-bd35-df5d64d21a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['Qwen2ForCausalLM'],\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 151643,\n",
       " 'eos_token_id': 151643,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 3584,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 18944,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'max_window_layers': 28,\n",
       " 'model_type': 'qwen2',\n",
       " 'num_attention_heads': 28,\n",
       " 'num_hidden_layers': 28,\n",
       " 'num_key_value_heads': 4,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'rope_theta': 1000000.0,\n",
       " 'sliding_window': 131072,\n",
       " 'tie_word_embeddings': False,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.37.2',\n",
       " 'use_cache': True,\n",
       " 'use_sliding_window': False,\n",
       " 'vocab_size': 152064}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Qwen2-7B/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587d42a-73a9-4a64-88b0-52533baacdcf",
   "metadata": {},
   "source": [
    "## We will use these configs to assemble the QWen2\n",
    "1. 28 transformer layers\n",
    "2. 28 attention heads\n",
    "3. 4 kv heads and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eeed7fa-af82-469e-b1ad-ba65db385055",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "n_kv_heads = config[\"num_key_value_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"rms_norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea783ba8-5770-4f03-ab8c-9f4dccd4f1a2",
   "metadata": {},
   "source": [
    "## Convert text to tokens\n",
    "I'm going to use QWen2's build-in tokenizer to do presentation.\n",
    "\n",
    "You may be confused why \"学习\" and \"，不\" is in one token (consider the principle of bpe). Later, some other Chinese characters maybe represented by two or more tokens like \"炊\".\n",
    "<div>\n",
    "    <img src=\"images/embedding_layers.png\", width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13da007c-c5c9-442e-a516-3d62ee2e78a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"学习如逆水行舟，不进则\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "q_len = len(tokens)\n",
    "q_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edef3b8b-5fd3-4a83-8a7f-a460a276a8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'学习如逆水行舟，不进则'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853fb782-7752-43d4-b6aa-56d689d38b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "471ef96d-8fc3-4f65-9c90-9fd9abd6718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0270, -0.0077, -0.0400,  ...,  0.0011,  0.0075,  0.0004],\n",
       "        [ 0.0225,  0.0047,  0.0073,  ...,  0.0013, -0.0077, -0.0337],\n",
       "        [-0.0303, -0.0083,  0.0029,  ...,  0.0033,  0.0057,  0.0061],\n",
       "        ...,\n",
       "        [ 0.0005,  0.0129, -0.0093,  ...,  0.0118,  0.0028,  0.0113],\n",
       "        [ 0.0112,  0.0210, -0.0214,  ..., -0.0061, -0.0099, -0.0027],\n",
       "        [ 0.0175,  0.0070, -0.0198,  ...,  0.0104,  0.0007, -0.0079]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = torch.nn.Embedding.from_pretrained(model['model.embed_tokens.weight'])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens)\n",
    "token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054bce7-402c-4caf-a199-87d237a34b23",
   "metadata": {},
   "source": [
    "## Normalize the embedding using rms normalization\n",
    "RMS normalization (Root Mean Square normalization) is used in the embedding layers of Large Language Models (LLMs) for several reasons:\n",
    "1. Stabilizing Training\n",
    "2. Improving Convergence\n",
    "3. Better Generalization\n",
    "4. Handling Variability in Embedding Magnitudes\n",
    "\n",
    "It's worth noting that we need to set a norm_eps to avoid the formula dived by 0.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/rms_norm.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64588068-1f7e-46df-97eb-3c837e501c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4c090-46d4-4ed3-a965-fc1ce24c1509",
   "metadata": {},
   "source": [
    "# Build the first transformer layer\n",
    "### Normalization\n",
    "You can see, after through layer0 from the dict extract from the model.\n",
    "\n",
    "The output tensor is still shape in [10*3584] but normalized.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/norm.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b76a3f-67dd-4366-a379-c6e5a0405799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5887, -0.1632, -0.8407,  ...,  0.0224,  0.1508,  0.0071],\n",
       "        [ 0.4845,  0.0988,  0.1508,  ...,  0.0276, -0.1528, -0.6674],\n",
       "        [-0.6847, -0.1818,  0.0634,  ...,  0.0732,  0.1187,  0.1274],\n",
       "        ...,\n",
       "        [ 0.0102,  0.2679, -0.1921,  ...,  0.2451,  0.0558,  0.2216],\n",
       "        [ 0.2462,  0.4459, -0.4507,  ..., -0.1303, -0.2004, -0.0538],\n",
       "        [ 0.3835,  0.1500, -0.4181,  ...,  0.2221,  0.0149, -0.1588]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model[\"model.layers.0.input_layernorm.weight\"])\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc6246-ce93-4ae8-879e-390e04122af0",
   "metadata": {},
   "source": [
    "## Assemble attention from scratch\n",
    "Load the attention heads of the first layer of transformer.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/qkv.png\", width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9dbcd2-b516-4e55-b998-712e2abede87",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer0 = model[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "k_layer0 = model[\"model.layers.0.self_attn.k_proj.weight\"]\n",
    "v_layer0 = model[\"model.layers.0.self_attn.v_proj.weight\"]\n",
    "o_layer0 = model[\"model.layers.0.self_attn.o_proj.weight\"]\n",
    "q_layer0_bias = model['model.layers.0.self_attn.q_proj.bias']\n",
    "k_layer0_bias = model['model.layers.0.self_attn.k_proj.bias']\n",
    "v_layer0_bias = model['model.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc492a89-0e9c-435b-96ca-76a365937d2a",
   "metadata": {},
   "source": [
    "## Now, we recive the query, key, and value for the token\n",
    "Their shape is [10*3584], which 10 is the length of embedding tokens and 3584 is dimension of hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d3af1d-b3df-420b-8903-0324e70d5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = torch.matmul(token_embeddings, q_layer0.T)+q_layer0_bias\n",
    "key_states = torch.matmul(token_embeddings, k_layer0.T)+k_layer0_bias\n",
    "value_states = torch.matmul(token_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c1baf0-e830-4265-8ff0-8d6181d964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = dim//n_heads\n",
    "query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd657fd-65b8-4e4b-b902-c33122ff3055",
   "metadata": {},
   "source": [
    "## Positioning encoding\n",
    "Due to query, key, and value can not represent the position information of tokens. Transformers are designed to handle sequences of data, but unlike recurrent neural networks (RNNs), they do not process the data in a sequential order. Positional encoding addresses this by adding information about the position of each token in the sequence, enabling the model to understand the order and relative position of tokens.\n",
    "\n",
    "### RoPE\n",
    "watch this video (this is what i watched) to understand the math.\n",
    "**https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s**\n",
    "\n",
    "### Here I use the original positional encoding code from QWen2\n",
    "Qwen2RotaryEmbedding() is used to generate rotating position encoding to efficiently provide position encoding for input sequences by calculating and caching cosine and sine values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "965d1db5-5fb2-46f8-a276-1ae5567a90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "rotary_emb = Qwen2RotaryEmbedding(\n",
    "            128,\n",
    "            max_position_embeddings=131072,\n",
    "            base=rope_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8e3f1-dd11-4dae-a827-ba67b6f657f1",
   "metadata": {},
   "source": [
    "## apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1)\n",
    "By combining query and key tensors with cosine and sine values, including rotation operations, positional coding information is embedded in the query and key tensors.\n",
    "## rotate_half(x)\n",
    "This rotation operation allows each element of the vector to be combined with the cosine and sine values of the corresponding position, thereby changing the direction and amplitude of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53e5a0b-aa06-473b-81cb-cbc9892b2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be770e30-e653-4b4b-84ed-110a19533660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "position_ids = torch.arange(q_len).view(1,q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b44c88a9-e06b-4a58-a4b7-62df0dd94668",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad4b29e6-d009-4aa0-b968-9d78fd06492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b7fb25-cb30-4a13-8da0-1393495b17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "value_states = repeat_kv(value_states, n_heads // n_kv_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79329733-28a8-466d-aa89-4d7e988d1312",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "This is a core attention mechanism in the Transformer architecture that allows the model to dynamically adjust its focus to different locations based on correlations in the input sequence. Specifically, this function performs dot product attention calculations on query, key, and value tensors.\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/softmax.png\", width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2174fff2-0989-4b2c-bfc9-7e5e37e47db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    key_states,\n",
    "    value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c69e00a9-2373-4937-b2d3-ed3eae4481b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.view(1, q_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3dd439-a258-48a6-9e1d-13829c94a078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8568e-01,  1.3149e-01, -1.5167e-01,  ...,  2.8487e-02,\n",
       "          -8.3742e-02, -2.3384e-02],\n",
       "         [-1.2537e-01,  2.0195e-01, -1.2300e-02,  ..., -3.6986e-02,\n",
       "          -1.8594e-01,  9.9794e-02],\n",
       "         [-1.4426e-01,  1.5807e-01, -1.7747e-01,  ..., -7.1516e-02,\n",
       "           7.0311e-02, -1.7331e-01],\n",
       "         ...,\n",
       "         [-5.9189e-02,  4.0363e-02, -1.3974e-05,  ..., -5.2831e-02,\n",
       "          -2.0385e-02,  8.6324e-03],\n",
       "         [ 3.7043e-02,  5.2902e-02,  3.0693e-03,  ..., -8.9145e-02,\n",
       "          -1.0277e-01,  1.0480e-02],\n",
       "         [-8.8573e-02,  1.8764e-02, -4.4170e-02,  ...,  1.4842e-01,\n",
       "          -9.0892e-02,  5.9852e-02]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_states = torch.matmul(attn_output, o_layer0.T)\n",
    "output_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cd220-64a2-408f-a644-87a717ee46df",
   "metadata": {},
   "source": [
    "## Residual neural networks\n",
    "The problem of vanishing gradient and exploding gradient can be solved by introducing residual block.\n",
    "<div>\n",
    "    <img src=\"images/add1.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4648d-938a-435c-9ca0-918decdc988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_states = output_states+token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5129ff6-263e-4ab5-9bc8-f5396e457e63",
   "metadata": {},
   "source": [
    "## Normalize and then run a feed forward neural network through the embedding delta\n",
    "<div>\n",
    "    <img src=\"images/norm2.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b06228-a58b-4dde-9f30-78e307c1f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_normalized = rms_norm(token_embeddings_unnormalized, model[\"model.layers.0.post_attention_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c6689-096f-4af2-9634-22ce3ee30369",
   "metadata": {},
   "source": [
    "## Loading the ff weights and implementing the ffn\n",
    "<div>\n",
    "    <img src=\"images/feedforward.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999c5c9-b451-4c32-aae7-c5b7bc18e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model[f\"model.layers.0.mlp.gate_proj.weight\"]\n",
    "w2 = model[f\"model.layers.0.mlp.down_proj.weight\"]\n",
    "w3 = model[f\"model.layers.0.mlp.up_proj.weight\"]\n",
    "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e1f8e-f41a-4cfe-8979-21cb5b6e1fd4",
   "metadata": {},
   "source": [
    "## Everything is done!!!~\n",
    "Now, run them at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a131a7af-666f-4c45-a5d5-3e093384a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "x= 0\n",
    "for layer in range(n_layers):\n",
    "    x+=1\n",
    "    residual1 = final_embedding\n",
    "    \n",
    "    # embeding norm\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"model.layers.{layer}.input_layernorm.weight\"])\n",
    "    \n",
    "    q_layer = model[f\"model.layers.{layer}.self_attn.q_proj.weight\"]\n",
    "    k_layer = model[f\"model.layers.{layer}.self_attn.k_proj.weight\"]\n",
    "    v_layer = model[f\"model.layers.{layer}.self_attn.v_proj.weight\"]\n",
    "    w_layer = model[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    q_layer_bias = model[f'model.layers.{layer}.self_attn.q_proj.bias']\n",
    "    k_layer_bias = model[f'model.layers.{layer}.self_attn.k_proj.bias']\n",
    "    v_layer_bias = model[f'model.layers.{layer}.self_attn.v_proj.bias']\n",
    "\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    head_dim = dim//n_heads\n",
    "    query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "    position_ids = torch.arange(q_len).view(1,q_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "    value_states = repeat_kv(value_states, n_heads // n_kv_heads)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(1, q_len, dim)\n",
    "    output_states = torch.matmul(attn_output, w_layer.T)\n",
    "        \n",
    "    hidden_state = residual1+output_states\n",
    "\n",
    "    # Fully connected\n",
    "    residual2 = hidden_state\n",
    "    \n",
    "    w1 = model[f\"model.layers.{layer}.mlp.gate_proj.weight\"]\n",
    "    w2 = model[f\"model.layers.{layer}.mlp.down_proj.weight\"]\n",
    "    w3 = model[f\"model.layers.{layer}.mlp.up_proj.weight\"]\n",
    "    second_normalized = rms_norm(hidden_state, model[f\"model.layers.{layer}.post_attention_layernorm.weight\"])\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)\n",
    "    final_embedding = residual2+output_after_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97719c73-4029-488a-88c1-2f58ace2b5e6",
   "metadata": {},
   "source": [
    "## Here is the final embedding\n",
    "The shape of it is same as the first embedding [10*3584].\n",
    "<div>\n",
    "    <img src=\"images/final_norm.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e319e392-14f4-4875-bb7e-6ab1ff3f9e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3584])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_normalized = rms_norm(final_embedding, model[\"model.norm.weight\"])\n",
    "final_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c490e12-f1e9-4656-b67c-0b18fa652994",
   "metadata": {},
   "source": [
    "## Finally!!! We can decode the embedding into the token value!\n",
    "<div>\n",
    "    <img src=\"images/final_linear.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a59710bf-2aa5-4018-8e45-393c1af9923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([152064])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.matmul(final_normalized[0][-1], model[\"lm_head.weight\"].T)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa370468-0279-4a77-ab89-259e082b068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55806])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits, dim=-1).view(1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775ab65-bf3f-4cdb-ad15-53ad8d1ad8fe",
   "metadata": {},
   "source": [
    "# Oh! yeah!~~~\n",
    "<div>\n",
    "    <img src=\"images/tui.png\" width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b543329b-3a85-4bf8-850e-2616583c6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'退'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afc986-9c4a-4e69-a7dd-5c25c9a5eaab",
   "metadata": {},
   "source": [
    "# Here I need to appreciate Naklecha's Llama3 work\n",
    "According to his **[Llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)**, I totally understand the structure of a decoder-only LLM.\n",
    "\n",
    "# In addition, I also want to broadcast Chinese LLM\n",
    "Performence of Qwen2 has improved so much comparing to the previous version. \n",
    "\n",
    "# Help LLM beginner\n",
    "Due to I'm not computer science graduates, I really meet so many problems. I hope my project can help these people who want to learn LLM.\n",
    "\n",
    "If you have any suggestions, plz and don't hesitate and contact me!!\n",
    "\n",
    "My RED num is 495668258 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29569b1d-46b8-4dfa-9676-78236e24c80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
