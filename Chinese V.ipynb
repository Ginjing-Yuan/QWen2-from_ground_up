{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2c7e6cd-6087-42eb-add7-0143210de63e",
   "metadata": {},
   "source": [
    "# 从头拆解千问2(QWen2)\n",
    "在这个项目中，我将演示如何从头开始拆解QWen2。具体来说，我将探索如何完成一个从输入input_text=“学习如逆水行舟，不进则”生成“退”。\n",
    "\n",
    "我希望这个项目能帮助大家更好地了解QWen2的结构，也希望借此机会推广中国的LLM。\n",
    "\n",
    "这里有千问2的魔搭社区下载地址: **https://www.modelscope.cn/models/qwen/Qwen2-7B/files**\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/all_steps.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d46041-7dbd-4d7c-bb14-2d7bbd7d9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3846ceb-a2d3-4104-9b8e-c1c56f6fd115",
   "metadata": {},
   "source": [
    "# 分词器\n",
    "这篇文章中我不对分词器做过多赘述。Andrej Karpathy已经将GPT4的Tokenizer做了详细简洁的复现。请参考他的[GitHub](https://github.com/karpathy/minbpe)。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/tokenizer.png\" width=500/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3116bf7-4777-4a76-be81-7ff8c52b1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520929d795234c9581ad98d167eaeaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To ensure every layer's output is same with model.generate(). The model should be load in precision of torch.float32!\n",
    "model_path=\"Qwen/Qwen2-7B\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a331a2b5-a2ba-4fbf-9f05-036a538313a6",
   "metadata": {},
   "source": [
    "# 读取模型文件\n",
    "通常情况下，我们直接使用Transformer加载大模型并进行推理。\n",
    "但是，这次为了拆解大模型，我将提取其中的每一层，逐层进行输出讲解。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/layers_dict.png\" width=500/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40fda03f-4ff6-4729-8f14-cf4ea4af5bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    \"model.embed_tokens.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.weight\",\n",
      "    \"model.layers.0.self_attn.q_proj.bias\",\n",
      "    \"model.layers.0.self_attn.k_proj.weight\",\n",
      "    \"model.layers.0.self_attn.k_proj.bias\",\n",
      "    \"model.layers.0.self_attn.v_proj.weight\",\n",
      "    \"model.layers.0.self_attn.v_proj.bias\",\n",
      "    \"model.layers.0.self_attn.o_proj.weight\",\n",
      "    \"model.layers.0.mlp.gate_proj.weight\",\n",
      "    \"model.layers.0.mlp.up_proj.weight\",\n",
      "    \"model.layers.0.mlp.down_proj.weight\",\n",
      "    \"model.layers.0.input_layernorm.weight\",\n",
      "    \"model.layers.0.post_attention_layernorm.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.weight\",\n",
      "    \"model.layers.1.self_attn.q_proj.bias\",\n",
      "    \"model.layers.1.self_attn.k_proj.weight\",\n",
      "    \"model.layers.1.self_attn.k_proj.bias\",\n",
      "    \"model.layers.1.self_attn.v_proj.weight\",\n",
      "    \"model.layers.1.self_attn.v_proj.bias\",\n",
      "    \"model.layers.1.self_attn.o_proj.weight\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "model = model.state_dict()\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22de6dab-0872-4adc-bd35-df5d64d21a66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'architectures': ['Qwen2ForCausalLM'],\n",
       " 'attention_dropout': 0.0,\n",
       " 'bos_token_id': 151643,\n",
       " 'eos_token_id': 151643,\n",
       " 'hidden_act': 'silu',\n",
       " 'hidden_size': 3584,\n",
       " 'initializer_range': 0.02,\n",
       " 'intermediate_size': 18944,\n",
       " 'max_position_embeddings': 131072,\n",
       " 'max_window_layers': 28,\n",
       " 'model_type': 'qwen2',\n",
       " 'num_attention_heads': 28,\n",
       " 'num_hidden_layers': 28,\n",
       " 'num_key_value_heads': 4,\n",
       " 'rms_norm_eps': 1e-06,\n",
       " 'rope_theta': 1000000.0,\n",
       " 'sliding_window': 131072,\n",
       " 'tie_word_embeddings': False,\n",
       " 'torch_dtype': 'bfloat16',\n",
       " 'transformers_version': '4.37.2',\n",
       " 'use_cache': True,\n",
       " 'use_sliding_window': False,\n",
       " 'vocab_size': 152064}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Qwen2-7B/config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587d42a-73a9-4a64-88b0-52533baacdcf",
   "metadata": {},
   "source": [
    "## 这里是千问2的一些基础参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eeed7fa-af82-469e-b1ad-ba65db385055",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = config[\"hidden_size\"]\n",
    "n_layers = config[\"num_hidden_layers\"]\n",
    "n_heads = config[\"num_attention_heads\"]\n",
    "n_kv_heads = config[\"num_key_value_heads\"]\n",
    "vocab_size = config[\"vocab_size\"]\n",
    "norm_eps = config[\"rms_norm_eps\"]\n",
    "rope_theta = torch.tensor(config[\"rope_theta\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea783ba8-5770-4f03-ab8c-9f4dccd4f1a2",
   "metadata": {},
   "source": [
    "## 将分词器结果转化为向量\n",
    "记得第一次面阿里的时候，他们问我嵌入层是如何训练的。我说嵌入层就是分词器--\\。现在想想当时多少有点傻。\n",
    "\n",
    "这里需要说明一点，有的token可以表示两个字，有的字需要一到两个token表示。所以，当预测的词需要两个token表示时，就需要将第一次生成的token加载原本的embedding text后，再进行一次输出了。\n",
    "<div>\n",
    "    <img src=\"images/embedding_layers.png\", width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13da007c-c5c9-442e-a516-3d62ee2e78a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"学习如逆水行舟，不进则\"\n",
    "tokens = tokenizer.encode(prompt)\n",
    "q_len = len(tokens)\n",
    "q_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edef3b8b-5fd3-4a83-8a7f-a460a276a8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'学习如逆水行舟，不进则'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853fb782-7752-43d4-b6aa-56d689d38b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "471ef96d-8fc3-4f65-9c90-9fd9abd6718a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0270, -0.0077, -0.0400,  ...,  0.0011,  0.0075,  0.0004],\n",
       "        [ 0.0225,  0.0047,  0.0073,  ...,  0.0013, -0.0077, -0.0337],\n",
       "        [-0.0303, -0.0083,  0.0029,  ...,  0.0033,  0.0057,  0.0061],\n",
       "        ...,\n",
       "        [ 0.0005,  0.0129, -0.0093,  ...,  0.0118,  0.0028,  0.0113],\n",
       "        [ 0.0112,  0.0210, -0.0214,  ..., -0.0061, -0.0099, -0.0027],\n",
       "        [ 0.0175,  0.0070, -0.0198,  ...,  0.0104,  0.0007, -0.0079]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = torch.nn.Embedding.from_pretrained(model['model.embed_tokens.weight'])\n",
    "token_embeddings_unnormalized = embedding_layer(tokens)\n",
    "token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8054bce7-402c-4caf-a199-87d237a34b23",
   "metadata": {},
   "source": [
    "## RMS归一化层\n",
    "归一化是为了增强表达和加速训练，这方面的基础知识可以参考吴恩达老师的机器学习相关课程。\n",
    "\n",
    "另外，为了保证分母不为0，我们需要设置一个norm_eps（这个参数在config文件中有，不同模型的这个参数有一定差别）。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/rms_norm.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64588068-1f7e-46df-97eb-3c837e501c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm(tensor, norm_weights):\n",
    "    return (tensor * torch.rsqrt(tensor.pow(2).mean(-1, keepdim=True) + norm_eps)) * norm_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a4c090-46d4-4ed3-a965-fc1ce24c1509",
   "metadata": {},
   "source": [
    "# 第一次归一化\n",
    "经过第一次归一化层后，向量形状仍为[10*3584]。\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/norm.png\", width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67b76a3f-67dd-4366-a379-c6e5a0405799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5887, -0.1632, -0.8407,  ...,  0.0224,  0.1508,  0.0071],\n",
       "        [ 0.4845,  0.0988,  0.1508,  ...,  0.0276, -0.1528, -0.6674],\n",
       "        [-0.6847, -0.1818,  0.0634,  ...,  0.0732,  0.1187,  0.1274],\n",
       "        ...,\n",
       "        [ 0.0102,  0.2679, -0.1921,  ...,  0.2451,  0.0558,  0.2216],\n",
       "        [ 0.2462,  0.4459, -0.4507,  ..., -0.1303, -0.2004, -0.0538],\n",
       "        [ 0.3835,  0.1500, -0.4181,  ...,  0.2221,  0.0149, -0.1588]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = rms_norm(token_embeddings_unnormalized, model[\"model.layers.0.input_layernorm.weight\"])\n",
    "token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc6246-ce93-4ae8-879e-390e04122af0",
   "metadata": {},
   "source": [
    "## 从头复现自注意力机制层\n",
    "\n",
    "<div>\n",
    "    <img src=\"images/qkv.png\", width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9dbcd2-b516-4e55-b998-712e2abede87",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_layer0 = model[\"model.layers.0.self_attn.q_proj.weight\"]\n",
    "k_layer0 = model[\"model.layers.0.self_attn.k_proj.weight\"]\n",
    "v_layer0 = model[\"model.layers.0.self_attn.v_proj.weight\"]\n",
    "o_layer0 = model[\"model.layers.0.self_attn.o_proj.weight\"]\n",
    "q_layer0_bias = model['model.layers.0.self_attn.q_proj.bias']\n",
    "k_layer0_bias = model['model.layers.0.self_attn.k_proj.bias']\n",
    "v_layer0_bias = model['model.layers.0.self_attn.v_proj.bias']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc492a89-0e9c-435b-96ca-76a365937d2a",
   "metadata": {},
   "source": [
    "## 获取向量的Q, K, V并重塑成可处理的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d3af1d-b3df-420b-8903-0324e70d5d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states = torch.matmul(token_embeddings, q_layer0.T)+q_layer0_bias\n",
    "key_states = torch.matmul(token_embeddings, k_layer0.T)+k_layer0_bias\n",
    "value_states = torch.matmul(token_embeddings, v_layer0.T)+v_layer0_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7c1baf0-e830-4265-8ff0-8d6181d964e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_dim = dim//n_heads\n",
    "query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd657fd-65b8-4e4b-b902-c33122ff3055",
   "metadata": {},
   "source": [
    "## 位置编码\n",
    "为了让单词之间知道自己的绝对位置和相对位置，需要引入旋转位置编码。\n",
    "\n",
    "### RoPE\n",
    "这里有RoPE的相关教学视频，此处不多做赘述。\n",
    "**https://www.youtube.com/watch?v=o29P0Kpobz0&t=530s**\n",
    "\n",
    "### 这里我直接提取了千问本身的位置编码代码。\n",
    "Qwen2RotaryEmbedding() 这个类的主要用途是为输入序列提供旋转位置编码，这种编码方式在 transformer 模型中用于处理序列数据。与传统的位置编码方法相比，RoPE 可以更好地捕捉相对位置信息，从而提高模型的性能。\n",
    "\n",
    "总结来说，这段代码实现了一个旋转位置编码的 PyTorch 模块，通过计算和缓存余弦和正弦值来高效地为输入序列提供位置编码。这在处理长序列数据时尤为有用，有助于模型更好地理解和处理序列中的位置信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "965d1db5-5fb2-46f8-a276-1ae5567a90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qwen2RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)\n",
    "\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos().to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin().to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[:seq_len].to(dtype=x.dtype),\n",
    "        )\n",
    "rotary_emb = Qwen2RotaryEmbedding(\n",
    "            128,\n",
    "            max_position_embeddings=131072,\n",
    "            base=rope_theta,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8e3f1-dd11-4dae-a827-ba67b6f657f1",
   "metadata": {},
   "source": [
    "## apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1)\n",
    "通过将查询和键张量与余弦和正弦值结合（包括旋转操作），使得位置编码信息嵌入到查询和键张量中。\n",
    "## rotate_half(x)\n",
    "这种旋转操作使得向量的每个元素能够和相应位置的余弦和正弦值相结合，进而改变向量的方向和幅度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d53e5a0b-aa06-473b-81cb-cbc9892b2574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):\n",
    "    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n",
    "\n",
    "    Args:\n",
    "        q (`torch.Tensor`): The query tensor.\n",
    "        k (`torch.Tensor`): The key tensor.\n",
    "        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n",
    "        sin (`torch.Tensor`): The sine part of the rotary embedding.\n",
    "        position_ids (`torch.Tensor`):\n",
    "            The position indices of the tokens corresponding to the query and key tensors. For example, this can be\n",
    "            used to pass offsetted position ids when working with a KV-cache.\n",
    "        unsqueeze_dim (`int`, *optional*, defaults to 1):\n",
    "            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n",
    "            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n",
    "            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n",
    "            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n",
    "            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n",
    "            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n",
    "    Returns:\n",
    "        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n",
    "    \"\"\"\n",
    "    cos = cos[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    sin = sin[position_ids].unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be770e30-e653-4b4b-84ed-110a19533660",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "position_ids = torch.arange(q_len).view(1,q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b44c88a9-e06b-4a58-a4b7-62df0dd94668",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad4b29e6-d009-4aa0-b968-9d78fd06492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85b7fb25-cb30-4a13-8da0-1393495b17c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "value_states = repeat_kv(value_states, n_heads // n_kv_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79329733-28a8-466d-aa89-4d7e988d1312",
   "metadata": {},
   "source": [
    "## 缩放点积注意力\n",
    "torch.nn.functional.scaled_dot_product_attention 是 PyTorch 提供的一个函数，用于计算缩放点积注意力（Scaled Dot-Product Attention）。这是 transformer 架构中核心的注意力机制，能够让模型根据输入序列中的相关性动态地调整对不同位置的关注。具体来说，这个函数对查询（query）、键（key）和值（value）张量执行点积注意力计算。\n",
    "<div>\n",
    "    <img src=\"images/softmax.png\", width=600>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2174fff2-0989-4b2c-bfc9-7e5e37e47db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "    query_states,\n",
    "    key_states,\n",
    "    value_states,\n",
    "    attn_mask=None,\n",
    "    dropout_p= 0.0,\n",
    "    # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "    is_causal= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c69e00a9-2373-4937-b2d3-ed3eae4481b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "attn_output = attn_output.view(1, q_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d3dd439-a258-48a6-9e1d-13829c94a078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8568e-01,  1.3149e-01, -1.5167e-01,  ...,  2.8487e-02,\n",
       "          -8.3742e-02, -2.3384e-02],\n",
       "         [-1.2537e-01,  2.0195e-01, -1.2300e-02,  ..., -3.6986e-02,\n",
       "          -1.8594e-01,  9.9794e-02],\n",
       "         [-1.4426e-01,  1.5807e-01, -1.7747e-01,  ..., -7.1516e-02,\n",
       "           7.0311e-02, -1.7331e-01],\n",
       "         ...,\n",
       "         [-5.9189e-02,  4.0363e-02, -1.3974e-05,  ..., -5.2831e-02,\n",
       "          -2.0385e-02,  8.6324e-03],\n",
       "         [ 3.7043e-02,  5.2902e-02,  3.0693e-03,  ..., -8.9145e-02,\n",
       "          -1.0277e-01,  1.0480e-02],\n",
       "         [-8.8573e-02,  1.8764e-02, -4.4170e-02,  ...,  1.4842e-01,\n",
       "          -9.0892e-02,  5.9852e-02]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_states = torch.matmul(attn_output, o_layer0.T)\n",
    "output_states"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231cd220-64a2-408f-a644-87a717ee46df",
   "metadata": {},
   "source": [
    "## 残差神经网络\n",
    "残差神经网络（ResNet）通过引入残差块（residual block），有效地解决了深度神经网络在训练过程中遇到的一些常见问题，特别是 梯度消失（vanishing gradient） 和 梯度爆炸（exploding gradient） 问题。\n",
    "<div>\n",
    "    <img src=\"images/add1.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4648d-938a-435c-9ca0-918decdc988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_states = output_states+token_embeddings_unnormalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5129ff6-263e-4ab5-9bc8-f5396e457e63",
   "metadata": {},
   "source": [
    "## 第二次归一化\n",
    "<div>\n",
    "    <img src=\"images/norm2.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b06228-a58b-4dde-9f30-78e307c1f9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_normalized = rms_norm(token_embeddings_unnormalized, model[\"model.layers.0.post_attention_layernorm.weight\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104c6689-096f-4af2-9634-22ce3ee30369",
   "metadata": {},
   "source": [
    "## 前馈神经网络（Feedforward Neural Network，FFNN）\n",
    "FFNN 由输入层、一个或多个隐藏层和输出层组成。每一层的神经元通过权重连接，每个神经元计算输入的加权和并应用激活函数。\n",
    "<div>\n",
    "    <img src=\"images/feedforward.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f999c5c9-b451-4c32-aae7-c5b7bc18e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = model[f\"model.layers.0.mlp.gate_proj.weight\"]\n",
    "w2 = model[f\"model.layers.0.mlp.down_proj.weight\"]\n",
    "w3 = model[f\"model.layers.0.mlp.up_proj.weight\"]\n",
    "output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9e1f8e-f41a-4cfe-8979-21cb5b6e1fd4",
   "metadata": {},
   "source": [
    "## Everything is done!!!~\n",
    "Now, run them at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a131a7af-666f-4c45-a5d5-3e093384a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embedding = token_embeddings_unnormalized\n",
    "x= 0\n",
    "for layer in range(n_layers):\n",
    "    x+=1\n",
    "    residual1 = final_embedding\n",
    "    \n",
    "    # embeding norm\n",
    "    layer_embedding_norm = rms_norm(final_embedding, model[f\"model.layers.{layer}.input_layernorm.weight\"])\n",
    "    \n",
    "    q_layer = model[f\"model.layers.{layer}.self_attn.q_proj.weight\"]\n",
    "    k_layer = model[f\"model.layers.{layer}.self_attn.k_proj.weight\"]\n",
    "    v_layer = model[f\"model.layers.{layer}.self_attn.v_proj.weight\"]\n",
    "    w_layer = model[f\"model.layers.{layer}.self_attn.o_proj.weight\"]\n",
    "    q_layer_bias = model[f'model.layers.{layer}.self_attn.q_proj.bias']\n",
    "    k_layer_bias = model[f'model.layers.{layer}.self_attn.k_proj.bias']\n",
    "    v_layer_bias = model[f'model.layers.{layer}.self_attn.v_proj.bias']\n",
    "\n",
    "    query_states = torch.matmul(layer_embedding_norm, q_layer.T)+q_layer_bias\n",
    "    key_states = torch.matmul(layer_embedding_norm, k_layer.T)+k_layer_bias\n",
    "    value_states = torch.matmul(layer_embedding_norm, v_layer.T)+v_layer_bias\n",
    "    head_dim = dim//n_heads\n",
    "    query_states = query_states.view(1, q_len, n_heads, head_dim).transpose(1, 2)\n",
    "    key_states = key_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "    value_states = value_states.view(1, q_len, n_kv_heads, head_dim).transpose(1, 2)\n",
    "\n",
    "    cos, sin = rotary_emb(value_states, seq_len=q_len)\n",
    "    position_ids = torch.arange(q_len).view(1,q_len)\n",
    "    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "    \n",
    "    key_states = repeat_kv(key_states, n_heads // n_kv_heads)\n",
    "    value_states = repeat_kv(value_states, n_heads // n_kv_heads)\n",
    "    \n",
    "    attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
    "        query_states,\n",
    "        key_states,\n",
    "        value_states,\n",
    "        attn_mask=None,\n",
    "        dropout_p= 0.0,\n",
    "        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n",
    "        is_causal= True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    attn_output = attn_output.view(1, q_len, dim)\n",
    "    output_states = torch.matmul(attn_output, w_layer.T)\n",
    "        \n",
    "    hidden_state = residual1+output_states\n",
    "\n",
    "    # Fully connected\n",
    "    residual2 = hidden_state\n",
    "    \n",
    "    w1 = model[f\"model.layers.{layer}.mlp.gate_proj.weight\"]\n",
    "    w2 = model[f\"model.layers.{layer}.mlp.down_proj.weight\"]\n",
    "    w3 = model[f\"model.layers.{layer}.mlp.up_proj.weight\"]\n",
    "    second_normalized = rms_norm(hidden_state, model[f\"model.layers.{layer}.post_attention_layernorm.weight\"])\n",
    "    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(second_normalized, w1.T)) * torch.matmul(second_normalized, w3.T), w2.T)\n",
    "    final_embedding = residual2+output_after_feedforward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97719c73-4029-488a-88c1-2f58ace2b5e6",
   "metadata": {},
   "source": [
    "## Here is the final embedding\n",
    "The shape of it is same as the first embedding [10*3584].\n",
    "<div>\n",
    "    <img src=\"images/final_norm.png\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e319e392-14f4-4875-bb7e-6ab1ff3f9e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 3584])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_normalized = rms_norm(final_embedding, model[\"model.norm.weight\"])\n",
    "final_normalized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c490e12-f1e9-4656-b67c-0b18fa652994",
   "metadata": {},
   "source": [
    "## Finally!!! We can decode the embedding into the token value!\n",
    "<div>\n",
    "    <img src=\"images/final_linear.png\" width=500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a59710bf-2aa5-4018-8e45-393c1af9923b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([152064])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.matmul(final_normalized[0][-1], model[\"lm_head.weight\"].T)\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fa370468-0279-4a77-ab89-259e082b068e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([55806])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token = torch.argmax(logits, dim=-1).view(1)\n",
    "next_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775ab65-bf3f-4cdb-ad15-53ad8d1ad8fe",
   "metadata": {},
   "source": [
    "# Oh! yeah!~~~\n",
    "<div>\n",
    "    <img src=\"images/tui.png\" width=500>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b543329b-3a85-4bf8-850e-2616583c6cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'退'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(next_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3afc986-9c4a-4e69-a7dd-5c25c9a5eaab",
   "metadata": {},
   "source": [
    "# 首先，我需要感谢 Naklecha的工作对我的启发，他让我更加透彻和深入的了解了大模型。\n",
    "According to his **[Llama3-from-scratch](https://github.com/naklecha/llama3-from-scratch)**, I totally understand the structure of a decoder-only LLM.\n",
    "\n",
    "# 其次，我也希望国产大模型能够在全世界发光发热。\n",
    "Performence of Qwen2 has improved so much comparing to the previous version. \n",
    "\n",
    "# 最后，由于我不是计算机专业科班出身，不管是从事算法工作还是什么，都经历了很多挫折，包括前文说的分词器和嵌入层的笑话。\n",
    "我希望借此机会帮助更多的初学者，对这篇文章有任何改进意见的朋友也欢迎小红书联系我。我的ID在下面。\n",
    "My RED num is 495668258 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29569b1d-46b8-4dfa-9676-78236e24c80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral",
   "language": "python",
   "name": "mistral"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
